{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1CWMOpxhvYw9CGaKwCQ/w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Sequence-to-Sequence (Seq2Seq) Models\n","\n","Sequence-to-sequence models are a type of recurrent neural network (RNN) architecture designed to transform one sequence into another. Unlike models that produce a single output (label) or take a single input and produce a sequence, **Seq2Seq models handle both variable-length input and output sequences.**\n","\n","They typically consist of two main components:\n","\n","* **Encoder:** Processes the input sequence and compresses it into a fixed-length vector representation (the \"context vector\" or \"thought vector\"). This vector aims to capture the essential information from the entire input sequence.\n","* **Decoder:** Takes the context vector as input and generates the output sequence, step by step. It uses the information in the context vector and previously generated outputs to predict the next element in the sequence.\n","\n","**Key characteristic:** Both input and output are sequences of potentially different lengths.\n","\n","---\n","\n","## Comparison with Label-to-Sequence and Sequence-to-Label Models\n","\n","Here's a brief comparison:\n","\n","| Model Type          | Input                      | Output                     | Use Cases                                                              |\n","| :------------------ | :------------------------- | :------------------------- | :----------------------------------------------------------------------- |\n","| **Sequence-to-Label** | Variable-length sequence   | Single label/classification | Sentiment analysis (text -> positive/negative), text categorization   |\n","| **Label-to-Sequence** | Single label/input vector  | Variable-length sequence   | Image captioning (image features -> sentence), music generation (genre -> melody) |\n","| **Sequence-to-Sequence** | Variable-length sequence   | Variable-length sequence   | Machine translation (English sentence -> French sentence), text summarization (long text -> short summary), chatbot responses |\n","\n","**In essence:**\n","\n","* **Sequence-to-Label:** Focuses on understanding the *entire* input sequence to make a single prediction.\n","* **Label-to-Sequence:** Generates a sequence based on a single starting point or concept.\n","* **Sequence-to-Sequence:** Deals with transforming a complex, ordered input into a new, ordered output, where the length and elements of both can vary.\n","\n","---\n","**Step 1: Sample Sentence**\n","\n","Let's consider the English sentence:\n"],"metadata":{"id":"P_ywnPEywhuI"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"WfvPD_k-qhuD","executionInfo":{"status":"ok","timestamp":1746154335512,"user_tz":360,"elapsed":16,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}}},"outputs":[],"source":["# Input and target sentence\n","input_sentence = \"how are you\"\n","output_sentence = \"cómo estás tú\""]},{"cell_type":"markdown","source":["**Step 2: Tokenization**\n","\n","Tokenization is the process of splitting a sentence into individual words (or subwords).\n","\n","It helps models process sequences word-by-word.\n","\n","For the phrase \"how are you\", the tokenized form would be:\n","\n","> `[\"how\", \"are\", \"you\"]`\n","\n","Each word becomes a separate token."],"metadata":{"id":"p2Ri35ZPxJAf"}},{"cell_type":"code","source":["input_tokens = input_sentence.lower().split()\n","output_tokens = output_sentence.lower().split()\n","print(\"Input Tokens:\", input_tokens)\n","print(\"Output Tokens:\", output_tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9r6webLxdqO","executionInfo":{"status":"ok","timestamp":1746154337265,"user_tz":360,"elapsed":17,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"781d3f41-0b89-4dfd-98e3-1f3d9f3c467a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Tokens: ['how', 'are', 'you']\n","Output Tokens: ['cómo', 'estás', 'tú']\n"]}]},{"cell_type":"markdown","source":["**Step 3: Vocabulary Creation (Word → Integer)**\n","\n","After tokenization, we create a vocabulary, which is a mapping from each unique token in our dataset to a unique integer. This allows the model to work with numerical representations instead of raw text.\n","\n","For our example tokens: `[\"how\", \"are\", \"you\"]`, a possible vocabulary could be:\n","\n","| Word | Integer |\n","|---|---|\n","| how | 1 |\n","| are | 2 |\n","| you | 3 |\n","\n","So, the tokenized sentence `[\"how\", \"are\", \"you\"]` would be represented as the integer sequence:\n","\n","> `[1, 2, 3]`"],"metadata":{"id":"nN33lia4xh-G"}},{"cell_type":"code","source":["def build_vocab(tokens):\n","    vocab = {word: idx + 1 for idx, word in enumerate(set(tokens))}\n","    vocab[\"<PAD>\"] = 0\n","    vocab[\"<BOS>\"] = len(vocab)\n","    vocab[\"<EOS>\"] = len(vocab)\n","    return vocab\n","\n","input_vocab = build_vocab(input_tokens)\n","output_vocab = build_vocab(output_tokens)\n","\n","print(\"Input Vocab:\", input_vocab)\n","print(\"Output Vocab:\", output_vocab)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uL6oIc-6xxd9","executionInfo":{"status":"ok","timestamp":1746154340746,"user_tz":360,"elapsed":8,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"3726ce8f-ad67-49a0-975f-7a218b4ec6a5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Vocab: {'are': 1, 'how': 2, 'you': 3, '<PAD>': 0, '<BOS>': 4, '<EOS>': 5}\n","Output Vocab: {'tú': 1, 'estás': 2, 'cómo': 3, '<PAD>': 0, '<BOS>': 4, '<EOS>': 5}\n"]}]},{"cell_type":"markdown","source":["**Step 4: Encode Sentences (Words → Numbers)**\n","\n","Now, we take our tokenized sentence and convert each token into its corresponding integer based on the vocabulary we created. We also typically add special tokens to signal the start and end of a sentence to the model.\n","\n","Using our vocabulary from Step 3 and adding `<BOS>` (Begin of Sentence) and `<EOS>` (End of Sentence) tokens (let's assume `<BOS>` maps to index 0 and `<EOS>` maps to index 4), the encoding process for our example sentence \"how are you\" would look like this:\n","\n","1. **Add `<BOS>` at the beginning:** `[\"<BOS>\", \"how\", \"are\", \"you\"]`\n","2. **Add `<EOS>` at the end:** `[\"<BOS>\", \"how\", \"are\", \"you\", \"<EOS>\"]`\n","3. **Map each token to its integer index:**\n","\n","   | Token | Integer |\n","   |---|---|\n","   | `<BOS>` | 0 |\n","   | how | 1 |\n","   | are | 2 |\n","   | you | 3 |\n","   | `<EOS>` | 4 |\n","\n","Therefore, the encoded representation of the sentence \"how are you\" becomes the numerical sequence:\n","\n","> `[0, 1, 2, 3, 4]`"],"metadata":{"id":"p41M7FQ6yHfc"}},{"cell_type":"code","source":["def encode_sentence(tokens, vocab, add_sos_eos=False):\n","    encoded = [vocab[word] for word in tokens]\n","    if add_sos_eos:\n","        return [vocab[\"<BOS>\"]] + encoded + [vocab[\"<EOS>\"]]\n","    return encoded\n","\n","input_encoded = encode_sentence(input_tokens, input_vocab)\n","output_encoded = encode_sentence(output_tokens, output_vocab, add_sos_eos=True)\n","\n","print(\"Encoded Input:\", input_encoded)\n","print(\"Encoded Output:\", output_encoded)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jM-ibnFwyJMd","executionInfo":{"status":"ok","timestamp":1746154343797,"user_tz":360,"elapsed":9,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"b2744322-bf34-4a1b-dde7-7acbe3f2f096"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded Input: [2, 1, 3]\n","Encoded Output: [4, 3, 2, 1, 5]\n"]}]},{"cell_type":"markdown","source":["**Step 5: Padding (to equal length for batching)**\n","\n","When training neural networks, it's often more efficient to process data in batches of sentences rather than one at a time. However, sentences in a dataset typically have varying lengths. To create batches of equal size, we use a technique called padding.\n","\n","Padding involves adding a special \"padding\" token (often `<PAD>`, let's assume it has an index of 5 in our vocabulary) to the end of shorter sequences until they reach the length of the longest sequence in the batch.\n","\n","**Example:**\n","\n","Let's say we have another encoded sentence:\n","\n","> `[0, 1, 4]`  (representing \"<BOS> cat <EOS>\")\n","\n","And our current encoded sentence is:\n","\n","> `[0, 1, 2, 3, 4]` (representing \"<BOS> how are you <EOS>\")\n","\n","If we want to batch these two sentences, the maximum length is 5. The shorter sentence needs to be padded:\n","\n","Original shorter sentence: `[0, 1, 4]`\n","\n","Padded shorter sentence (to length 5): `[0, 1, 4, 5, 5]`\n","\n","Now both sentences have the same length and can be processed together in a batch:\n","\n","> `[[0, 1, 2, 3, 4],`\n",">\n","> ` [0, 1, 4, 5, 5]]`\n","\n","The `<PAD>` tokens are usually masked or ignored by the model during training so they don't contribute to the learning process."],"metadata":{"id":"qPtZe1j6ySub"}},{"cell_type":"code","source":["def pad_sequence(seq, max_len):\n","    return seq + [0] * (max_len - len(seq))\n","\n","max_input_len = 5\n","max_output_len = 6\n","\n","input_padded = pad_sequence(input_encoded, max_input_len)\n","output_padded = pad_sequence(output_encoded, max_output_len)\n","\n","print(\"Padded Input:\", input_padded)\n","print(\"Padded Output:\", output_padded)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rsSMwt47yX6-","executionInfo":{"status":"ok","timestamp":1746154352006,"user_tz":360,"elapsed":50,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"ef76777e-b710-4c6a-dd84-e998f329952b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Padded Input: [2, 1, 3, 0, 0]\n","Padded Output: [4, 3, 2, 1, 5, 0]\n"]}]},{"cell_type":"markdown","source":["**Step 6: One-Hot Encoding**\n","\n","While the integer encoding from the previous steps provides a numerical representation, neural networks often work better with one-hot encoded vectors, especially for categorical data like word indices.\n","\n","One-hot encoding transforms each integer into a binary vector where only the index corresponding to the integer has a value of 1, and all other indices have a value of 0. The length of the vector is equal to the size of the vocabulary (including special tokens like `<BOS>`, `<EOS>`, and `<PAD>`).\n","\n","**Example (using our vocabulary of size 6: {`<BOS>`: 0, `how`: 1, `are`: 2, `you`: 3, `<EOS>`: 4, `<PAD>`: 5}):**\n","\n","Let's take our padded, encoded sentence: `[0, 1, 2, 3, 4]` (we'll ignore the padded example for simplicity in this step).\n","\n","Each integer in this sequence will be converted into a 6-dimensional one-hot vector:\n","\n","* **0** (`<BOS>`) becomes: `[1, 0, 0, 0, 0, 0]`\n","* **1** (`how`) becomes: `[0, 1, 0, 0, 0, 0]`\n","* **2** (`are`) becomes: `[0, 0, 1, 0, 0, 0]`\n","* **3** (`you`) becomes: `[0, 0, 0, 1, 0, 0]`\n","* **4** (`<EOS>`) becomes: `[0, 0, 0, 0, 1, 0]`\n","\n","So, the one-hot encoded representation of the sentence \"how are you\" would be a sequence of these vectors:\n","\n","> `[[1, 0, 0, 0, 0, 0],`  (`<BOS>`)\n",">\n","> ` [0, 1, 0, 0, 0, 0],`  (`how`)\n",">\n","> ` [0, 0, 1, 0, 0, 0],`  (`are`)\n",">\n","> ` [0, 0, 0, 1, 0, 0],`  (`you`)\n",">\n","> ` [0, 0, 0, 0, 1, 0]]` (`<EOS>`)\n","\n","Each word in the input sequence is now represented by a sparse vector that the neural network can process. While effective, for large vocabularies, one-hot encoding can be memory-intensive, which is why other embedding techniques are often used in practice."],"metadata":{"id":"LS7iyUoyyaPP"}},{"cell_type":"code","source":["import numpy as np\n","\n","def one_hot_encode(seq, vocab_size):\n","    one_hot = np.zeros((len(seq), vocab_size))\n","    for t, val in enumerate(seq):\n","        one_hot[t, val] = 1\n","    return one_hot\n","\n","input_vocab_size = len(input_vocab)\n","output_vocab_size = len(output_vocab)\n","\n","input_oh = one_hot_encode(input_padded, input_vocab_size)\n","output_oh = one_hot_encode(output_padded, output_vocab_size)\n","\n","print(\"One-hot Encoded Input Shape:\", input_oh.shape)\n","print(\"One-hot Encoded Output Shape:\", output_oh.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpjJKWQoygZN","executionInfo":{"status":"ok","timestamp":1746154374856,"user_tz":360,"elapsed":11,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"c8fcfd22-e7ae-49d5-f530-0953748a536f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["One-hot Encoded Input Shape: (5, 6)\n","One-hot Encoded Output Shape: (6, 6)\n"]}]},{"cell_type":"markdown","source":["## Recurrent Neural Networks (RNNs)\n","\n","**Why Do We Need Memory in Models?**\n","\n","Traditional neural networks, such as those built with simple feedforward layers, process each input independently without considering the order or context of previous inputs in a sequence. This limitation makes them unsuitable for tasks where the order of elements is crucial for understanding meaning.\n","\n","**For example:**\n","\n","Consider these two sentences:\n","\n","* Sentence A: \"The cat ate the food.\"\n","* Sentence B: \"The food ate the cat.\"\n","\n","Both sentences consist of the exact same words. However, the different order of these words completely changes the meaning of the sentences. A feedforward network, processing each word in isolation, would struggle to differentiate between these two semantically distinct phrases.\n","\n","**RNNs are designed to remember previous inputs while processing the current input.** They achieve this through a mechanism called a **hidden state**.\n","\n","At each step of processing a sequence, an RNN takes the current input and the previous hidden state as input. It then produces an output for the current step and updates its hidden state. This updated hidden state carries information about the history of the sequence encountered so far, effectively acting as the model's \"memory\". This allows RNNs to understand the relationships and dependencies between elements in a sequence, making them well-suited for tasks involving sequential data like text, time series, and audio.\n","\n","---\n","**Example Sentence for RNN**\n","\n","Let's use a simple toy sentence to illustrate how an RNN might process it:"],"metadata":{"id":"tEnyvgMz2BZQ"}},{"cell_type":"code","source":["sentence = input(\"Enter a sentence: \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ul3bLTAA2EqQ","executionInfo":{"status":"ok","timestamp":1746154414009,"user_tz":360,"elapsed":4909,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"4a977a86-6184-4a68-9b4d-9f8aac5af9b5"},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter a sentence: hello there\n"]}]},{"cell_type":"markdown","source":["**Step 1: Tokenization**\n","\n"],"metadata":{"id":"VvvopN5D2fAy"}},{"cell_type":"code","source":["tokens = sentence.lower().split()\n","print(\"Tokens:\", tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6bWrQrT2pLo","executionInfo":{"status":"ok","timestamp":1746154416257,"user_tz":360,"elapsed":15,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"ec17aeed-5989-4941-899a-ec37564ec332"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens: ['hello', 'there']\n"]}]},{"cell_type":"markdown","source":["**Step 2: Vocabulary with `<BOS>` and `<EOS>`**\n","\n","Before encoding, we define our vocabulary, including the special tokens for the beginning and end of the sentence. For our example, let's say our vocabulary and their corresponding integer indices are:\n"],"metadata":{"id":"mQb_-Qyg2sic"}},{"cell_type":"code","source":["def build_vocab_with_special(tokens):\n","    vocab = {word: idx + 1 for idx, word in enumerate(set(tokens))}\n","    vocab[\"<PAD>\"] = 0\n","    vocab[\"<BOS>\"] = len(vocab)\n","    vocab[\"<EOS>\"] = len(vocab)\n","    return vocab\n","\n","vocab = build_vocab_with_special(tokens)\n","print(\"Vocab:\", vocab)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lx7ywmat220r","executionInfo":{"status":"ok","timestamp":1746154418320,"user_tz":360,"elapsed":33,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"d53b2d3a-7651-466c-e524-7a3cc1a0daff"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab: {'there': 1, 'hello': 2, '<PAD>': 0, '<BOS>': 3, '<EOS>': 4}\n"]}]},{"cell_type":"markdown","source":["**Step 3: Encode with `<BOS>` and `<EOS>`**\n","\n","Now, we take our tokenized sentence and encode it using the vocabulary we defined in Step 2, including the `<BOS>` and `<EOS>` tokens:\n","\n"],"metadata":{"id":"GVHmca5226eM"}},{"cell_type":"code","source":["def encode_with_bos_eos(tokens, vocab):\n","    return [vocab[\"<BOS>\"]] + [vocab[t] for t in tokens] + [vocab[\"<EOS>\"]]\n","\n","encoded = encode_with_bos_eos(tokens, vocab)\n","print(\"Encoded Sentence:\", encoded)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbXkv6na3Axt","executionInfo":{"status":"ok","timestamp":1746154419977,"user_tz":360,"elapsed":10,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"5c205c5a-d4ee-435b-e6a8-223ac5a1b41a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded Sentence: [3, 2, 1, 4]\n"]}]},{"cell_type":"markdown","source":["**Step 4: One-hot Encoding for Input**"],"metadata":{"id":"biHN4fg93Crz"}},{"cell_type":"code","source":["def one_hot_encode(seq, vocab_size):\n","    one_hot = np.zeros((len(seq), vocab_size))\n","    for t, val in enumerate(seq):\n","        one_hot[t, val] = 1\n","    return one_hot\n","\n","vocab_size = len(vocab)\n","input_seq = one_hot_encode(encoded, vocab_size)\n","print(\"Input Shape:\", input_seq.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b9vq7Rg3MYF","executionInfo":{"status":"ok","timestamp":1746154422447,"user_tz":360,"elapsed":11,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"8e90819b-96ce-42b6-c1b5-c4bda358f751"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Shape: (4, 5)\n"]}]},{"cell_type":"markdown","source":["**Step 5: The RNN Forward Pass — Why Do We Need It?**\n","\n","**First, Recall the Two Sentences:**\n","\n","* Sentence A: \"The cat ate the food.\"\n","* Sentence B: \"The food ate the cat.\"\n","\n","These two sentences have exactly the same words, just in a different order. But the meaning is completely different — in A, the cat is eating; in B, the food is eating (which is nonsense!).\n","\n","So the model **must understand order**.\n","\n","**What Happens if We Don’t Use an RNN?**\n","\n","If we use a normal feedforward neural network, each word is treated independently — it won’t know what came before or after.\n","\n","That means:\n","Markdown\n","\n","**Step 5: The RNN Forward Pass — Why Do We Need It?**\n","\n","**First, Recall the Two Sentences:**\n","\n","* Sentence A: \"The cat ate the food.\"\n","* Sentence B: \"The food ate the cat.\"\n","\n","These two sentences have exactly the same words, just in a different order. But the meaning is completely different — in A, the cat is eating; in B, the food is eating (which is nonsense!).\n","\n","So the model **must understand order**.\n","\n","**What Happens if We Don’t Use an RNN?**\n","\n","If we use a normal feedforward neural network, each word is treated independently — it won’t know what came before or after.\n","\n","That means:\n","\n","\"cat\" -> output vector\n","\n","\"ate\" -> output vector\n","\n","\"food\" -> output vector\n","\n","\n","But it won’t know if \"cat\" came before \"food\" or after.\n","\n","**So it treats Sentence A and Sentence B as the same!**\n","\n","**What Does Step 5 (RNN Forward Pass) Do?**\n","\n","RNN introduces **memory**.\n","\n","At every time step $t$, it computes the hidden state $h_t$ based on the current input $x_t$ and the previous hidden state $h_{t-1}$:\n","\n","$$h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b)$$\n","\n","That means:\n","\n","* $x_t$ is the current word at time $t$ (e.g., \"the\", \"cat\", etc.)\n","* $h_{t-1}$ is the memory from the previous step — the context\n","\n","**So:**\n","\n","When the RNN sees the word “ate”, its interpretation depends on what it saw before:\n","\n","* In Sentence A: it saw “the cat” $\\rightarrow$ “ate” likely means the subject is “cat”\n","* In Sentence B: it saw “the food” $\\rightarrow$ “ate” likely means the subject is “food”\n","\n","**Example Flow (in Intuition)**\n","\n","**Sentence A: \"The cat ate the food\"**\n","\n","| Step | Word  | Hidden state ($h_t$) remembers |\n","|------|-------|------------------------------|\n","| 1    | \"The\" | Start of sentence            |\n","| 2    | \"cat\" | \"The cat\"                    |\n","| 3    | \"ate\" | \"The cat ate\"                |\n","| 4    | \"the\" | \"The cat ate the\"            |\n","| 5    | \"food\"| \"The cat ate the food\"       |\n","\n","**Sentence B: \"The food ate the cat\"**\n","\n","| Step | Word  | Hidden state ($h_t$) remembers |\n","|------|-------|------------------------------|\n","| 1    | \"The\" | Start of sentence            |\n","| 2    | \"food\"| \"The food\"                   |\n","| 3    | \"ate\" | \"The food ate\"               |\n","| 4    | \"the\" | \"The food ate the\"           |\n","| 5    | \"cat\" | \"The food ate the cat\"       |\n","\n","**So What Does Step 5 Do?**\n","\n","It creates a **unique hidden state for every word, based on its position and history.**\n","\n","This is what helps the RNN understand sequence and context — so it can tell the difference between:\n","\n","> \"The cat ate the food\"\n",">\n","> \"The food ate the cat\"\n","\n","Without this, the model would see just a bag of words and get totally confused."],"metadata":{"id":"5nlt3x_15AMS"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Hyperparameters\n","input_dim = vocab_size\n","hidden_dim = 4  # for simplicity\n","\n","# Weights and bias\n","np.random.seed(42)\n","W_xh = np.random.randn(input_dim, hidden_dim)\n","W_hh = np.random.randn(hidden_dim, hidden_dim)\n","b = np.zeros((hidden_dim,))\n","\n","def rnn_forward(inputs):\n","    h_t = np.zeros((hidden_dim,))\n","    hidden_states = []\n","\n","    for x_t in inputs:\n","        h_t = np.tanh(np.dot(x_t, W_xh) + np.dot(h_t, W_hh) + b)\n","        hidden_states.append(h_t.copy())\n","\n","    return np.array(hidden_states)\n","\n","hidden_states = rnn_forward(input_seq)\n","print(\"Hidden States Shape:\", hidden_states.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTEznIvJ54LZ","executionInfo":{"status":"ok","timestamp":1746154429267,"user_tz":360,"elapsed":23,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"324bb799-7c04-4128-9c87-05f0ac9fd086"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Hidden States Shape: (4, 4)\n"]}]},{"cell_type":"markdown","source":["**Visualize Hidden States**"],"metadata":{"id":"zYYU8C6a594w"}},{"cell_type":"code","source":["for t, h in enumerate(hidden_states):\n","    print(f\"Step {t}: {h}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3fFMp406Bix","executionInfo":{"status":"ok","timestamp":1746154431362,"user_tz":360,"elapsed":10,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"89dfa710-3445-4d26-e6c5-71d685d57bac"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 0: [ 0.23734834 -0.95736006 -0.93845248 -0.50967271]\n","Step 1: [ 0.74875492  0.83230964  0.66401981 -0.9792767 ]\n","Step 2: [ 0.02453996  0.48632465 -0.48786326  0.9848857 ]\n","Step 3: [-0.74511185 -0.49053645 -0.34737921 -0.99764036]\n"]}]},{"cell_type":"code","source":["# Real Seq2Seq Translation with Transformers (English → French)\n","\n","!pip install -q transformers sentencepiece\n"],"metadata":{"id":"DRJNjW-yHZZt","executionInfo":{"status":"ok","timestamp":1746157825972,"user_tz":360,"elapsed":3406,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from transformers import MarianMTModel, MarianTokenizer\n","\n","# Load Pretrained English → French Model\n","model_name = 'Helsinki-NLP/opus-mt-en-fr'\n","tokenizer = MarianTokenizer.from_pretrained(model_name)\n","model = MarianMTModel.from_pretrained(model_name)\n","\n","\n","# Translate Function\n","def translate_real(sentence):\n","    print(f\"Input: {sentence}\")\n","    tokens = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n","    output = model.generate(**tokens)\n","    translated = tokenizer.decode(output[0], skip_special_tokens=True)\n","    print(f\"Output: {translated}\\n\")\n","\n","\n","# Try Demo Sentences\n","translate_real(\"I am a student.\")\n","translate_real(\"The cat ate the food.\")\n","translate_real(\"The food ate the cat.\")\n","translate_real(\"I love computer science.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZoHxcMMrHLVh","executionInfo":{"status":"ok","timestamp":1746157838096,"user_tz":360,"elapsed":6752,"user":{"displayName":"Gnanitha Garikipati","userId":"03590086070442967660"}},"outputId":"d4551bd3-5930-48e4-bd5f-174c9ff4fbe5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: I am a student.\n","Output: Je suis étudiant.\n","\n","Input: The cat ate the food.\n","Output: Le chat a mangé la nourriture.\n","\n","Input: The food ate the cat.\n","Output: La nourriture a mangé le chat.\n","\n","Input: I love computer science.\n","Output: J'adore l'informatique.\n","\n"]}]}]}