{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanism Lab with Multihead Self-Attention in PyTorch\n",
        "\n",
        "### What Is Attention?\n",
        "\n",
        "Before diving into code, let’s understand why we need attention.\n",
        "\n",
        "In NLP (Natural Language Processing), models must decide which parts of a sentence are important to look at. Attention lets each token focus more on relevant tokens in the sequence.\n",
        "\n",
        "For example:\n",
        "\n",
        "* In the sentence “The food was hot because it was spicy”, the word “it” should attend to “food”.\n",
        "\n",
        "### Understanding Input Dimensions\n",
        "\n",
        "We’re working with 3D tensors in this lab:\n",
        "\n",
        "* **B**: Batch size (how many sentences we process at once)\n",
        "* **T**: Number of tokens in each sentence\n",
        "* **C**: Number of features (embeddings) for each token\n",
        "\n",
        "###Part 1: Implementing a Basic Self-Attention Head\n",
        "\n",
        "### What’s a Self-Attention Head?\n",
        "\n",
        "A single attention head is a mechanism that allows a token to:\n",
        "\n",
        "* **Query (Q)**: Ask a question (e.g., what am I looking for?)\n",
        "* **Key (K)**: Answer if it has what the query wants\n",
        "* **Value (V)**: Send information if the key matches\n",
        "\n",
        "Tokens compare queries and keys using dot products and weigh each token’s value based on these scores."
      ],
      "metadata": {
        "id": "k59vCbIDUZlF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArWwBvePUXXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f5143b-7097-4b7b-a2d1-80ef6de1a7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input x (batch 0):\n",
            " tensor([[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433],\n",
            "        [ 1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086]])\n",
            "\n",
            "Query q (batch 0):\n",
            " tensor([[ 0.0110, -0.1263,  0.4046,  0.0332,  0.2670, -0.1756,  0.1955, -0.0063],\n",
            "        [ 0.6874, -0.0661, -0.2071,  0.3671,  0.6184, -0.5196,  0.4594, -0.2400],\n",
            "        [ 0.0537,  0.4173,  0.5473,  0.2127, -0.3923,  0.7439,  0.6044, -0.9145],\n",
            "        [-0.2882,  0.1333,  0.5673,  1.1221,  0.8594,  0.3488,  0.5764, -0.1420]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n",
            "Key k (batch 0):\n",
            " tensor([[ 0.1337,  0.1884,  0.2608,  0.0804, -0.4088,  0.2697, -0.0554,  0.2315],\n",
            "        [ 0.7615,  0.6625, -0.1835,  0.1656,  0.8969,  0.4457, -0.2870,  0.5668],\n",
            "        [-0.6749, -0.9168, -0.1317,  0.0897,  1.0475,  0.3863,  1.2753, -0.6169],\n",
            "        [ 0.2671, -0.1097, -0.6221,  0.8526,  0.1161,  0.8693,  1.3773,  1.5398]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n",
            "Value v (batch 0):\n",
            " tensor([[-0.4604,  0.4809,  0.1541,  0.3029, -0.4975,  0.1626, -0.3648, -0.0215],\n",
            "        [-0.4165,  0.1133,  0.0795, -0.0579,  0.2416,  0.4003, -0.1591, -0.6411],\n",
            "        [-1.2082, -1.5025, -0.3400, -0.2360, -0.4184, -0.2758,  0.1823,  1.3053],\n",
            "        [-1.6469, -0.6138,  0.2256,  1.5595, -0.9673,  0.7251,  0.0859, -0.0745]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n",
            "Attention Scores (batch 0):\n",
            " tensor([[-0.0293,    -inf,    -inf,    -inf],\n",
            "        [-0.1481,  0.2240,    -inf,    -inf],\n",
            "        [ 0.1278, -0.1626,  0.2615,    -inf],\n",
            "        [-0.0344,  0.2230,  0.6915,  0.5269]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Attention Weights (batch 0):\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4080, 0.5920, 0.0000, 0.0000],\n",
            "        [0.3459, 0.2587, 0.3954, 0.0000],\n",
            "        [0.1636, 0.2116, 0.3381, 0.2868]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Final Output (batch 0):\n",
            " tensor([[-0.4604,  0.4809,  0.1541,  0.3029, -0.4975,  0.1626, -0.3648, -0.0215],\n",
            "        [-0.4344,  0.2633,  0.1099,  0.0893, -0.0600,  0.3033, -0.2430, -0.3883],\n",
            "        [-0.7447, -0.3984, -0.0606, -0.0035, -0.2750,  0.0508, -0.0953,  0.3428],\n",
            "        [-1.0441, -0.5813, -0.0082,  0.4047, -0.4491,  0.2260, -0.0071,  0.2807]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# B = batch, T = token length, C = channel/feature size\n",
        "B, T, C = 2, 4, 8\n",
        "x = torch.randn(B, T, C)  # Random input simulating embedded tokens\n",
        "\n",
        "# Learnable linear layers for projecting to Q, K, V\n",
        "key = nn.Linear(C, C, bias=False)\n",
        "query = nn.Linear(C, C, bias=False)\n",
        "value = nn.Linear(C, C, bias=False)\n",
        "\n",
        "# Generate Q, K, V from input\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "v = value(x)\n",
        "\n",
        "# Compute raw attention scores by dot product of Q and K\n",
        "scores = q @ k.transpose(-2, -1) / C**0.5\n",
        "\n",
        "# Mask to prevent attending to future tokens (for decoder-style attention)\n",
        "mask = torch.tril(torch.ones(T, T))\n",
        "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "# Normalize scores into probabilities\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "# Weighted sum of values based on attention weights\n",
        "out = weights @ v\n",
        "\n",
        "# Print intermediate results (only for the first sample for brevity)\n",
        "print(\"Input x (batch 0):\\n\", x[0])\n",
        "print(\"\\nQuery q (batch 0):\\n\", q[0])\n",
        "print(\"\\nKey k (batch 0):\\n\", k[0])\n",
        "print(\"\\nValue v (batch 0):\\n\", v[0])\n",
        "print(\"\\nAttention Scores (batch 0):\\n\", scores[0])\n",
        "print(\"\\nAttention Weights (batch 0):\\n\", weights[0])\n",
        "print(\"\\nFinal Output (batch 0):\\n\", out[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Making It a Reusable Class\n",
        "\n",
        "Now let’s put the above logic into a reusable PyTorch module called `Head`. This will represent one attention head."
      ],
      "metadata": {
        "id": "R6-_TnUbVhPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, C_head):\n",
        "        super().__init__()\n",
        "        # Define linear projections for keys, queries, and values\n",
        "        self.key = nn.Linear(C_head, C_head, bias=False)     # Projects input to 'key' vector\n",
        "        self.query = nn.Linear(C_head, C_head, bias=False)   # Projects input to 'query' vector\n",
        "        self.value = nn.Linear(C_head, C_head, bias=False)   # Projects input to 'value' vector\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x: shape [B, T, C]\n",
        "        B, T, C = x.shape  # B: batch size, T: sequence length, C: embedding dimension\n",
        "\n",
        "        # Linear projections\n",
        "        k = self.key(x)    # [B, T, C] → key vectors\n",
        "        q = self.query(x)  # [B, T, C] → query vectors\n",
        "        v = self.value(x)  # [B, T, C] → value vectors\n",
        "\n",
        "        # Compute raw attention scores via scaled dot product\n",
        "        # scores[i][j] = similarity between query i and key j\n",
        "        # Shape: [B, T, T] where each score[i][j] is q[i] ⋅ k[j]\n",
        "        scores = q @ k.transpose(-2, -1) / C**0.5  # Scale by sqrt(C) to stabilize gradients\n",
        "\n",
        "        # Apply causal (lower triangular) mask to prevent attending to future tokens\n",
        "        # Ensures token i only attends to token j ≤ i (causal self-attention)\n",
        "        mask = torch.tril(torch.ones(T, T))  # Shape: [T, T]\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))  # Set future token scores to -inf\n",
        "\n",
        "        # Softmax to normalize scores into attention weights\n",
        "        # Each row of 'weights' sums to 1: it tells how much attention token i pays to j\n",
        "        weights = F.softmax(scores, dim=-1)  # Shape: [B, T, T]\n",
        "\n",
        "        # Final output: each token is a weighted sum of the value vectors\n",
        "        # Attention-weighted sum across time dimension (T)\n",
        "        out = weights @ v  # Shape: [B, T, C]\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "2WVPB6MxVkXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Multihead Attention\n",
        "\n",
        "### Why Multiple Heads?\n",
        "\n",
        "Each head learns a different perspective:\n",
        "\n",
        "* One may focus on grammar\n",
        "* Another on meaning\n",
        "* Another on position\n",
        "\n",
        "Multihead attention allows parallel learning from multiple viewpoints.\n"
      ],
      "metadata": {
        "id": "1qICMQuUVmbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, C, num_heads):\n",
        "        super().__init__()\n",
        "        assert C % num_heads == 0, \"Embedding dimension must be divisible by num_heads\"\n",
        "\n",
        "        # Create a list of independent attention heads\n",
        "        # Each head processes a portion of the input features\n",
        "        self.C = C\n",
        "        self.num_heads = num_heads\n",
        "        self.C_head = C // num_heads\n",
        "\n",
        "        self.heads = nn.ModuleList([\n",
        "            Head(self.C_head) for _ in range(num_heads)\n",
        "        ])\n",
        "        self.proj = nn.Linear(C, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, C]\n",
        "        head_outputs = []\n",
        "        for i, head in enumerate(self.heads):\n",
        "            # Slice feature dimension for each head: [B, T, C_head]\n",
        "            x_split = x[..., i * self.C_head : (i + 1) * self.C_head]\n",
        "            head_outputs.append(head(x_split))\n",
        "\n",
        "        # Concatenate along feature dimension: [B, T, C]\n",
        "        out = torch.cat(head_outputs, dim=-1)\n",
        "        return self.proj(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "aLMRRaR_Vyir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Simple Language Model Using Attention\n",
        "\n",
        "We’ll now build a basic language model that learns to predict the next character given the previous ones.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "* **Token Embedding**: Turns characters into vectors\n",
        "* **Positional Encoding**: Tells the model where each token is\n",
        "* **Multihead Attention**: Helps tokens attend to previous ones\n",
        "* **Output Layer**: Predicts the next token"
      ],
      "metadata": {
        "id": "J4zdtBgKV1VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding for tokens (words or subwords)\n",
        "        # Maps each token index to a learnable n_embd-dimensional vector\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "        # Embedding for positions (e.g., positions 0, 1, ..., block_size-1)\n",
        "        # Gives the model a sense of order in the sequence\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # Multi-head self-attention block (causal)\n",
        "        # Learns dependencies between tokens at different positions\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head)\n",
        "\n",
        "        # Final projection layer to output logits over the vocabulary\n",
        "        self.output = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # idx: [B, T] — input token indices (B = batch size, T = sequence length)\n",
        "\n",
        "        B, T = idx.shape  # Extract dimensions\n",
        "\n",
        "        # Token embeddings: shape [B, T, n_embd]\n",
        "        tok = self.token_emb(idx)\n",
        "\n",
        "        # Positional embeddings: shape [T, n_embd], then broadcast to [B, T, n_embd]\n",
        "        pos = self.pos_emb(torch.arange(T, device=idx.device))\n",
        "\n",
        "        # Combine token and position embeddings\n",
        "        x = tok + pos  # Shape: [B, T, n_embd]\n",
        "\n",
        "        # Pass through multi-head self-attention block\n",
        "        x = self.attn(x)  # Shape: [B, T, n_embd]\n",
        "\n",
        "        # Final linear layer to map embeddings to vocabulary logits\n",
        "        logits = self.output(x)  # Shape: [B, T, vocab_size]\n",
        "\n",
        "        return logits  # These logits are used to compute softmax and loss externally"
      ],
      "metadata": {
        "id": "WLCJDXPPV9CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: Training on Simple Text\n",
        "\n",
        "Let’s train this model on a very small example: \"hello world\"."
      ],
      "metadata": {
        "id": "2Pex6QBNV_aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hello world\"  # The input text corpus for training a toy language model\n",
        "\n",
        "# Extract all unique characters, sort them (just for consistency)\n",
        "chars = sorted(set(text))  # [' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']\n",
        "\n",
        "# Create a mapping from character to integer (index)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}  # \"string to index\"\n",
        "itos = {i: ch for ch, i in stoi.items()}       # \"index to string\"\n",
        "\n",
        "# Define encoding and decoding functions\n",
        "encode = lambda s: [stoi[c] for c in s]        # Converts string → list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Converts list of integers → string\n",
        "\n",
        "# Convert the entire string into a tensor of token indices\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "# For \"hello world\", this will look like: tensor([3, 2, 4, 4, 5, 0, 7, 5, 6, 4, 1])\n",
        "\n",
        "def get_batch(batch_size=4):\n",
        "    # Randomly pick 4 starting indices for 8-token long sequences\n",
        "    ix = torch.randint(0, len(data) - 8, (4,))\n",
        "    # For each index `i`, get a chunk of 8 tokens as input\n",
        "    x = torch.stack([data[i:i+8] for i in ix])  # Shape: [4, 8]\n",
        "\n",
        "    # For each input chunk, the corresponding target is the next 8 tokens\n",
        "    y = torch.stack([data[i+1:i+9] for i in ix])  # Shape: [4, 8]\n",
        "\n",
        "    return x, y  # x is input, y is expected output (next character)\n"
      ],
      "metadata": {
        "id": "gqkEvO53WHLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6: Training Loop\n",
        "\n",
        "Train the model on small sequences of characters:"
      ],
      "metadata": {
        "id": "LSL4pqCRWPWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleLanguageModel(len(chars), block_size=8, n_embd=16, n_head=2).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(200):\n",
        "    x, y = get_batch(batch_size=64)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "pXabej3XWQ5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be75d03d-00ea-4d1d-c96c-7d5e92827e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 2.1783\n",
            "Step 50, Loss: 1.5632\n",
            "Step 100, Loss: 0.8687\n",
            "Step 150, Loss: 0.3847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 7: Text Generation\n",
        "\n",
        "Generate new text starting from a seed character.\n",
        "\n",
        "Try changing the seed charecter to understand the text generation mechanism."
      ],
      "metadata": {
        "id": "RIXjUlv5WXpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, start_str, max_new_tokens=100):\n",
        "    model.eval()\n",
        "\n",
        "    # Encode the start string into tensor of token indices\n",
        "    idx = torch.tensor([encode(start_str)], dtype=torch.long).to(device)  # Shape: [1, T]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # If the input is longer than block_size, truncate from the left\n",
        "        idx_cond = idx[:, -8:]\n",
        "\n",
        "        # Get model predictions\n",
        "        logits = model(idx_cond)  # [1, T, vocab_size]\n",
        "        last_logits = logits[:, -1, :]  # Take the last time step's logits: [1, vocab_size]\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = F.softmax(last_logits, dim=-1)  # [1, vocab_size]\n",
        "\n",
        "        # Sample the next token (greedy or random)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)  # [1, 1]\n",
        "\n",
        "        # Append the next token to the sequence\n",
        "        idx = torch.cat([idx, next_token], dim=1)  # [1, T+1]\n",
        "\n",
        "    # Decode the entire sequence to string\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "print(generate(model, start_str=\"w\", max_new_tokens=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2VAGYpLRdRm",
        "outputId": "cfdb3e5b-bf7e-4aab-b6d7-a6370e09c0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrllo worldorlhld llwllwllorlhrllllldorldorlhld ldorlorl llllllllo lo worldlllrlhlworldorlrllldorlllo\n"
          ]
        }
      ]
    }
  ]
}