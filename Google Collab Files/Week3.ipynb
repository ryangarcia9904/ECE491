{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Bayes' Theorem\n",
        "\n",
        "Bayes’ theorem describes the probability of an event based on prior knowledge of conditions related to the event:\n",
        "\n",
        "$$ P(A | B) = \\frac{P(B | A) P(A)}{P(B)} $$\n",
        "\n",
        "**Example: Diagnosing a Disease**\n",
        "\n",
        "* $P(D) = 0.01$ (Probability of having the disease)\n",
        "* $P(\\neg D) = 0.99$ (Probability of not having the disease)\n",
        "* $P(T | D) = 0.95$ (Probability of testing positive given disease)\n",
        "* $P(T | \\neg D) = 0.05$ (False positive rate)\n",
        "\n",
        "Using Bayes’ theorem, the probability that a person who tested positive actually has the disease is:\n",
        "\n",
        "$$ P(D | T) = \\frac{P(T | D) P(D)}{P(T | D) P(D) + P(T | \\neg D) P(\\neg D)} $$\n",
        "\n",
        "$$ = \\frac{(0.95 \\times 0.01)}{(0.95 \\times 0.01) + (0.05 \\times 0.99)} = 0.161 $$"
      ],
      "metadata": {
        "id": "3__fg5PHKVLd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CYVIo3QGYyN"
      },
      "outputs": [],
      "source": [
        "# Given probabilities\n",
        "P_D = 0.01\n",
        "P_not_D = 0.99\n",
        "P_T_given_D = 0.95\n",
        "P_T_given_not_D = 0.05\n",
        "\n",
        "# Bayes' Theorem calculation\n",
        "P_D_given_T = (P_T_given_D * P_D) / (P_T_given_D * P_D + P_T_given_not_D * P_not_D)\n",
        "print(f\"P(D | T) = {P_D_given_T:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different Types of Probability Distributions in Deep Learning\n",
        "\n",
        "Probability distributions play a fundamental role in deep learning for modeling uncertainties, defining priors in Bayesian networks, and understanding data behavior. Below are important distributions with mathematical definitions and deep learning applications.\n",
        "\n",
        "**1. Bernoulli Distribution (Binary Outcomes)**\n",
        "\n",
        "* Used for modeling binary events (e.g., success/failure, 0/1).\n",
        "* Probability mass function (PMF):\n",
        "\n",
        "    $$ P(X = x) = p^x (1 - p)^{(1-x)} $$\n",
        "\n",
        "    where $x \\in \\{0, 1\\}$, and $p$ is the probability of success.\n",
        "\n",
        "* **Deep Learning Application:**\n",
        "    * Used in binary classification problems and dropout regularization."
      ],
      "metadata": {
        "id": "zibDUfYRKbXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import bernoulli\n",
        "\n",
        "p = 0.5  # Probability of success\n",
        "x = [0, 1]\n",
        "y = bernoulli.pmf(x, p)\n",
        "\n",
        "plt.bar(x, y)\n",
        "plt.title(\"Bernoulli Distribution (p=0.5)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CMhpQVYwKteL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Binomial Distribution**\n",
        "\n",
        "* Describes the number of successes in $n$ independent Bernoulli trials.\n",
        "* Probability mass function (PMF):\n",
        "\n",
        "    $$ P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k} $$\n",
        "\n",
        "* **Deep Learning Application:**\n",
        "    * Used in classification models where multiple independent trials occur."
      ],
      "metadata": {
        "id": "eo6HNWphKvGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import binom\n",
        "\n",
        "n, p = 10, 0.5\n",
        "x = np.arange(0, n + 1)\n",
        "y = binom.pmf(x, n, p)\n",
        "\n",
        "plt.bar(x, y)\n",
        "plt.title(\"Binomial Distribution (n=10, p=0.5)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jvDT4eBRK1eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Normal (Gaussian) Distribution**\n",
        "\n",
        "* Defined by mean $\\mu$ and variance $\\sigma^2$.\n",
        "* Probability density function (PDF):\n",
        "\n",
        "    $$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
        "\n",
        "* **Deep Learning Application:**\n",
        "    * Used for weight initialization and Bayesian deep learning."
      ],
      "metadata": {
        "id": "KrkjisgFK9bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "mu, sigma = 0, 1\n",
        "x = np.linspace(-4, 4, 100)\n",
        "y = norm.pdf(x, mu, sigma)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Normal Distribution (mu=0, sigma=1)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ETZj9clOK29L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Poisson Distribution**\n",
        "\n",
        "* Models the number of events occurring in a fixed interval.\n",
        "* Probability mass function:\n",
        "\n",
        "    $$ P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\n",
        "\n",
        "* **Deep Learning Application:**\n",
        "    * Used in event prediction models (e.g., customer arrivals in reinforcement learning)."
      ],
      "metadata": {
        "id": "Ak8dYNNSLBrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import poisson\n",
        "\n",
        "lambda_ = 5\n",
        "x = np.arange(0, 15)\n",
        "y = poisson.pmf(x, lambda_)\n",
        "\n",
        "plt.bar(x, y)\n",
        "plt.title(\"Poisson Distribution (lambda=5)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VO-rHcZqLJcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Exponential Distribution**\n",
        "\n",
        "* Describes the time until an event occurs.\n",
        "* Probability density function:\n",
        "\n",
        "    $$ f(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0 $$\n",
        "\n",
        "* **Deep Learning Application:**\n",
        "    * Used in queueing models and failure rate analysis."
      ],
      "metadata": {
        "id": "G83tglpaLO1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import expon\n",
        "\n",
        "lambda_ = 1\n",
        "x = np.linspace(0, 5, 100)\n",
        "y = expon.pdf(x, scale=1/lambda_)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Exponential Distribution (lambda=1)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DpzrFJC1LQf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Softmax Distribution**\n",
        "\n",
        "* Converts a vector into probabilities that sum to 1.\n",
        "* Formula:\n",
        "\n",
        "    $$ P(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
        "\n",
        "* **Deep Learning Application:**\n",
        "    * Used in the output layer of neural networks for multi-class classification."
      ],
      "metadata": {
        "id": "vIj7Q_c4LRlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Numerical stability\n",
        "    return exp_x / exp_x.sum()\n",
        "\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "y = softmax(x)\n",
        "\n",
        "print(f\"Softmax Probabilities: {y}\")\n"
      ],
      "metadata": {
        "id": "4uNnyPa7LbHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expectation and Variance\n",
        "\n",
        "**Expectation:**\n",
        "\n",
        "$$ E[X] = \\sum x P(X = x) $$\n",
        "\n",
        "**Variance:**\n",
        "\n",
        "$$ Var(X) = E[X^2] - (E[X])^2 $$\n",
        "\n",
        "**Example: Discrete Random Variable**\n",
        "\n",
        "Consider a random variable $X$ representing the outcome of rolling a fair 6-sided die:\n",
        "\n",
        "$$ X = \\{1, 2, 3, 4, 5, 6\\} $$\n",
        "\n",
        "Each outcome occurs with probability:\n",
        "\n",
        "$$ P(X = x) = \\frac{1}{6}, \\quad \\text{for } x \\in \\{1, 2, 3, 4, 5, 6\\} $$\n",
        "\n",
        "**Step 1: Compute Expectation $E[X]$**\n",
        "\n",
        "Expectation is given by:\n",
        "\n",
        "$$ E[X] = \\sum x P(X = x) $$\n",
        "\n",
        "Substituting values:\n",
        "\n",
        "$$ E[X] = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6} $$\n",
        "\n",
        "$$ = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5 $$\n",
        "\n",
        "**Step 2: Compute $E[X^2]$**\n",
        "\n",
        "$$ E[X^2] = \\sum x^2 P(X = x) $$\n",
        "\n",
        "$$ E[X^2] = 1^2 \\cdot \\frac{1}{6} + 2^2 \\cdot \\frac{1}{6} + 3^2 \\cdot \\frac{1}{6} + 4^2 \\cdot \\frac{1}{6} + 5^2 \\cdot \\frac{1}{6} + 6^2 \\cdot \\frac{1}{6} $$\n",
        "\n",
        "$$ = \\frac{1+4+9+16+25+36}{6} = \\frac{91}{6} = 15.167 $$\n",
        "\n",
        "**Step 3: Compute Variance $Var(X)$**\n",
        "\n",
        "$$ Var(X) = E[X^2] - (E[X])^2 $$\n",
        "\n",
        "$$ = 15.167 - (3.5)^2 $$\n",
        "\n",
        "$$ = 15.167 - 12.25 = 2.917 $$"
      ],
      "metadata": {
        "id": "q7zy8uZSLguD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5, 6])\n",
        "P = np.ones_like(X) / len(X)  # Uniform probability\n",
        "\n",
        "E_X = np.sum(X * P)  # Expectation\n",
        "E_X2 = np.sum(X**2 * P)  # Expectation of X^2\n",
        "Var_X = E_X2 - E_X**2  # Variance\n",
        "\n",
        "print(f\"Expectation (E[X]): {E_X}\")\n",
        "print(f\"Expectation of X^2 (E[X^2]): {E_X2}\")\n",
        "print(f\"Variance (Var(X)): {Var_X}\")\n"
      ],
      "metadata": {
        "id": "vdAf3EpYLpZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov Chains and Transition Matrix Explanation\n",
        "\n",
        "A Markov Chain is a stochastic process where the probability of transitioning to a future state depends only on the current state and not on past states. This property is known as the Markov Property (memoryless property).\n",
        "\n",
        "**Understanding the Transition Matrix P**\n",
        "\n",
        "Given the transition matrix:\n",
        "\n",
        "$$ P = \\begin{bmatrix} 0.7 & 0.3 \\\\ 0.4 & 0.6 \\end{bmatrix} $$\n",
        "\n",
        "This matrix represents probabilities of moving between two states (e.g., State 1 and State 2).\n",
        "\n",
        "* Each row represents the current state, and each column represents the next state.\n",
        "* Each row must sum to 1 because it represents probabilities of transitioning from one state to others.\n",
        "\n",
        "**Interpreting the Transition Probabilities**\n",
        "\n",
        "$$ P = \\begin{bmatrix} 0.7 & 0.3 \\\\ 0.4 & 0.6 \\end{bmatrix} $$\n",
        "\n",
        "* **Row 1 (Current State = 1):**\n",
        "    * 0.7 → Probability of staying in State 1.\n",
        "    * 0.3 → Probability of transitioning from State 1 to State 2.\n",
        "* **Row 2 (Current State = 2):**\n",
        "    * 0.4 → Probability of transitioning from State 2 to State 1.\n",
        "    * 0.6 → Probability of staying in State 2.\n",
        "\n",
        "**Example Calculation: One-Step Transition**\n",
        "\n",
        "Let's say the initial state probabilities are:\n",
        "\n",
        "$$ s_0 = \\begin{bmatrix} 1 & 0 \\end{bmatrix} $$\n",
        "\n",
        "This means we start in State 1 with 100% certainty.\n",
        "\n",
        "After one step, the new state probabilities are:\n",
        "\n",
        "$$ s_1 = s_0 P = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 0.7 & 0.3 \\\\ 0.4 & 0.6 \\end{bmatrix} $$\n",
        "\n",
        "$$ = \\begin{bmatrix} (1 \\times 0.7 + 0 \\times 0.4) & (1 \\times 0.3 + 0 \\times 0.6) \\end{bmatrix} $$\n",
        "\n",
        "$$ = \\begin{bmatrix} 0.7 & 0.3 \\end{bmatrix} $$\n",
        "\n",
        "Thus, after one step:\n",
        "\n",
        "* 70% probability of being in State 1\n",
        "* 30% probability of being in State 2"
      ],
      "metadata": {
        "id": "rBHfC3hzMB01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Transition matrix\n",
        "P = np.array([[0.7, 0.3],\n",
        "              [0.4, 0.6]])\n",
        "\n",
        "# Initial state (100% probability in State 1)\n",
        "s0 = np.array([1, 0])\n",
        "\n",
        "# Compute next state probabilities\n",
        "s1 = np.dot(s0, P)\n",
        "\n",
        "print(f\"State probabilities after one step: {s1}\")\n"
      ],
      "metadata": {
        "id": "NIPuVcpBMhR0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}